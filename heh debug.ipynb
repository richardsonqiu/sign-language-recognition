{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "  random.seed(seed)\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "  np.random.seed(seed)\n",
    "  tf.random.set_seed(seed)\n",
    "\n",
    "SEED = 22\n",
    "seed_everything(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 132, 1536, 1599, 1662]\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from itertools import accumulate\n",
    "\n",
    "landmark_lens = (\n",
    "    (33, 4),\n",
    "    (468, 3),\n",
    "    (21, 3),\n",
    "    (21, 3)\n",
    ")\n",
    "landmark_locs = list(accumulate(landmark_lens, lambda a, b: a + b[0]*b[1], initial=0))\n",
    "landmarks_len = reduce(lambda r, loc: r + loc[0] * loc[1], landmark_lens, 0)\n",
    "print(landmark_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label in os.listdir('tracks_binary') if os.path.isdir(f'tracks_binary/{label}')]\n",
    "NUM_CLASSES = len(labels)\n",
    "\n",
    "labels_tensor = tf.constant(labels)\n",
    "ids_tensor = tf.constant(range(len(labels)))\n",
    "\n",
    "ids_from_labels = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        labels_tensor,\n",
    "        ids_tensor\n",
    "    ),\n",
    "    default_value=-1\n",
    ")\n",
    "\n",
    "labels_from_ids = tf.lookup.StaticHashTable(\n",
    "    tf.lookup.KeyValueTensorInitializer(\n",
    "        ids_tensor,\n",
    "        labels_tensor\n",
    "    ),\n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "def to_categorical(label):\n",
    "    return tf.one_hot(\n",
    "        ids_from_labels.lookup(label),\n",
    "        depth=NUM_CLASSES\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Create Tensor\n",
    "tensor1 = tf.range(5)\n",
    "\n",
    "#Create dataset, this will return object of TensorSliceDataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset after applying batch and repeat\n",
      "tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor([2 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor([4 0], shape=(2,), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([3 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor([2 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor([4 0], shape=(2,), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([3 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor([2 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor([4 0], shape=(2,), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([3 4], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset after applying batch and repeat\")\n",
    "dataset = dataset.repeat(6).batch(batch_size=2)\n",
    "for i in dataset:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.reshape(tf.range(0.0, 16.0), [-1, 2, 4])\n",
    "b = tf.reshape(tf.range(10.0, 26.0), [-1, 2, 4])\n",
    "c = tf.reshape(tf.range(20.0, 36.0), [-1, 2, 4])\n",
    "d = tf.reshape(tf.range(30.0, 46.0), [-1, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n",
       "array([[[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.]],\n",
       "\n",
       "       [[ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.]]], dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n",
       "array([[[30., 31., 32., 33.],\n",
       "        [34., 35., 36., 37.]],\n",
       "\n",
       "       [[38., 39., 40., 41.],\n",
       "        [42., 43., 44., 45.]]], dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n",
       " array([[[ 2.1000001,  2.3      ,  2.       ,  3.       ],\n",
       "         [ 2.5      ,  2.7      ,  6.       ,  7.       ]],\n",
       " \n",
       "        [[ 2.9      ,  3.1000001, 10.       , 11.       ],\n",
       "         [ 3.3      ,  3.5      , 14.       , 15.       ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n",
       " array([[[ 3.1000001,  3.3      , 12.       , 13.       ],\n",
       "         [ 3.5      ,  3.7      , 16.       , 17.       ]],\n",
       " \n",
       "        [[ 3.9      ,  4.1      , 20.       , 21.       ],\n",
       "         [ 4.3      ,  4.5      , 24.       , 25.       ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n",
       " array([[[ 4.1      ,  4.3      , 22.       , 23.       ],\n",
       "         [ 4.5      ,  4.7000003, 26.       , 27.       ]],\n",
       " \n",
       "        [[ 4.9      ,  5.1      , 30.       , 31.       ],\n",
       "         [ 5.3      ,  5.5      , 34.       , 35.       ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n",
       " array([[[ 5.1      ,  5.3      , 32.       , 33.       ],\n",
       "         [ 5.5      ,  5.7000003, 36.       , 37.       ]],\n",
       " \n",
       "        [[ 5.9      ,  6.1      , 40.       , 41.       ],\n",
       "         [ 6.3      ,  6.5      , 44.       , 45.       ]]], dtype=float32)>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import reduce_max, reduce_min\n",
    "def calc_bounding(pose, face, lh, rh):\n",
    "  max_x = max(reduce_max(pose[:, :, :1]), reduce_max(face[:, :, :1]), reduce_max(lh[:, :, :1]), reduce_max(rh[:, :, :1]))\n",
    "  min_x = min(reduce_min(pose[:, :, :1]), reduce_min(face[:, :, :1]), reduce_min(lh[:, :, :1]), reduce_min(rh[:, :, :1]))\n",
    "\n",
    "  max_y = max(reduce_max(pose[:, :, 1:2]), reduce_max(face[:, :, 1:2]), reduce_max(lh[:, :, 1:2]), reduce_max(rh[:, :, 1:2]))\n",
    "  min_y = min(reduce_min(pose[:, :, 1:2]), reduce_min(face[:, :, 1:2]), reduce_min(lh[:, :, 1:2]), reduce_min(rh[:, :, 1:2]))\n",
    "\n",
    "  diff = (max_x - min_x, max_y - min_y)\n",
    "  mid = ((max_x + min_x)/2, (max_y + min_y)/2)\n",
    "  return (diff, mid)\n",
    "\n",
    "def scale(scale, pose, face, lh , rh):\n",
    "  scale = tf.cast(scale, dtype=tf.float32)\n",
    "  diff, mid = calc_bounding(pose, face, lh, rh)\n",
    "  pose_shape, face_shape, lh_shape, rh_shape = tf.shape(pose), tf.shape(face), tf.shape(lh), tf.shape(rh)\n",
    "  pose_add = tf.tile([[[mid[0], mid[1], 0, 0]]], [pose_shape[0], pose_shape[1], 1])\n",
    "  face_add = tf.tile([[[mid[0], mid[1], 0, 0]]], [face_shape[0], face_shape[1], 1])\n",
    "  lh_add = tf.tile([[[mid[0], mid[1], 0, 0]]], [lh_shape[0], lh_shape[1], 1])\n",
    "  rh_add = tf.tile([[[mid[0], mid[1], 0, 0]]], [rh_shape[0], rh_shape[1], 1])\n",
    "  \n",
    "  pose_scale = tf.tile([[[scale, scale, 1, 1]]], [pose_shape[0], pose_shape[1], 1])\n",
    "  face_scale = tf.tile([[[scale, scale, 1, 1]]], [face_shape[0], face_shape[1], 1])\n",
    "  lh_scale = tf.tile([[[scale, scale, 1, 1]]], [lh_shape[0], lh_shape[1], 1])\n",
    "  rh_scale = tf.tile([[[scale, scale, 1, 1]]], [rh_shape[0], rh_shape[1], 1])\n",
    "\n",
    "  scaled_pose = (pose + pose_add) * pose_scale\n",
    "  scaled_face = (face + face_add) * face_scale\n",
    "  scaled_lh = (lh + lh_add) * lh_scale\n",
    "  scaled_rh = (rh + rh_add) * rh_scale\n",
    "\n",
    "  return (scaled_pose, scaled_face, scaled_lh, scaled_rh)\n",
    "\n",
    "scale(0.1, a, b, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 1), dtype=float32, numpy=\n",
       "array([[[3.],\n",
       "        [5.]],\n",
       "\n",
       "       [[7.],\n",
       "        [9.]]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid = 6\n",
    "scale = 0.5\n",
    "z = tf.cast(a[:, :, :1] + mid, dtype=tf.float32) * scale\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 4), dtype=float32, numpy=\n",
       "array([[[ 3. ,  4.5,  0. ,  0. ],\n",
       "        [ 5. ,  6.5,  0. ,  0. ]],\n",
       "\n",
       "       [[ 7. ,  8.5,  0. ,  0. ],\n",
       "        [ 9. , 10.5,  0. ,  0. ]]], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid = (6, 8)\n",
    "s = tf.shape(a)\n",
    "add = tf.tile([[[mid[0], mid[1], 0, 0]]], [s[0], s[1], 1])\n",
    "scl = tf.tile([[[scale, scale, 0, 0]]], [s[0], s[1], 1])\n",
    "(tf.cast(a, dtype=tf.float32) + tf.cast(add, dtype=tf.float32)) * scl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 4), dtype=int32, numpy=\n",
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7]],\n",
       "\n",
       "       [[ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]]])>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\n",
       "array([[[31],\n",
       "        [35]],\n",
       "\n",
       "       [[39],\n",
       "        [43]]])>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[:, :, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=12>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(reduce_max(a[:, :, :1]), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_split(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=1000):\n",
    "  assert (train_split + test_split + val_split) == 1\n",
    "  \n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(shuffle_size, seed=SEED)\n",
    "  \n",
    "  train_size = int(train_split * ds_size)\n",
    "  val_size = int(val_split * ds_size)\n",
    "  \n",
    "  train_ds = ds.take(train_size)\n",
    "  val_ds = ds.skip(train_size).take(val_size)\n",
    "  test_ds = ds.skip(train_size).skip(val_size)\n",
    "  \n",
    "  return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = get_ds_split(dataset, len(dataset), shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor([2 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor([4 0], shape=(2,), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([3 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor([2 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor([4 0], shape=(2,), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([3 4], shape=(2,), dtype=int32)\n",
      "tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor([2 3], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i in train_ds:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4 0], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i in val_ds:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([3 4], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i in test_ds:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_binary(file_path):\n",
    "    label = tf.strings.split(file_path, os.sep)[-2]\n",
    "\n",
    "    raw = tf.io.read_file(file_path)\n",
    "    data = tf.io.decode_raw(raw, tf.float32)\n",
    "    data = tf.reshape(data, [-1, landmarks_len])\n",
    "\n",
    "    pose = tf.reshape(data[:, 0:132], [-1, 33, 4])\n",
    "    face = tf.reshape(data[:, 132:1536], [-1, 468, 3])\n",
    "    lh = tf.reshape(data[:, 1536:1599], [-1, 21, 3])\n",
    "    rh = tf.reshape(data[:, 1599:1662], [-1, 21, 3])\n",
    "\n",
    "    return (pose, face, lh, rh), to_categorical(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMES = 64\n",
    "\n",
    "def flatten(x):\n",
    "    pose = tf.reshape(x[0], shape=[-1, 132])\n",
    "    face = tf.reshape(x[1], shape=[-1, 1404])\n",
    "    lh = tf.reshape(x[2], shape=[-1, 63])\n",
    "    rh = tf.reshape(x[3], shape=[-1, 63])\n",
    "\n",
    "    return tf.concat([pose, face, lh, rh], axis=1)\n",
    "\n",
    "\n",
    "def random_window(x):\n",
    "    def pad(x):\n",
    "        missing = FRAMES - size\n",
    "        start_pad = tf.math.ceil(missing / 2)\n",
    "        end_pad = tf.math.floor(missing / 2)\n",
    "        return tf.concat([\n",
    "            tf.tile([x[0]], [start_pad, 1]),\n",
    "            x,\n",
    "            tf.tile([x[-1]], [end_pad, 1])\n",
    "        ], axis=0)\n",
    "\n",
    "    def random_slice(x):\n",
    "        i = tf.random.uniform(shape=(), maxval=size+1-FRAMES, dtype=tf.int32)\n",
    "        return x[i: i+FRAMES]\n",
    "\n",
    "    size = tf.shape(x)[0]\n",
    "    return tf.cond(\n",
    "        size < FRAMES,\n",
    "        lambda: pad(x),\n",
    "        lambda: random_slice(x)\n",
    "    )\n",
    "    \n",
    "\n",
    "def prepare(ds):\n",
    "    ds = ds.map(lambda x, y: (flatten(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    ds = ds.map(lambda x, y: (random_window(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    ds = ds.shuffle(1000, seed=SEED)\n",
    "\n",
    "    ds = ds.batch(32)\n",
    "\n",
    "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "ds = tf.data.Dataset.list_files('tracks_binary/*/*')\n",
    "ds = ds.map(process_binary)\n",
    "\n",
    "ds = ds.shuffle(1000, seed=SEED)\n",
    "\n",
    "dataset_size = len(ds)\n",
    "train_size = math.floor(0.8 * dataset_size)\n",
    "\n",
    "train_ds = ds.take(train_size)\n",
    "# train_ds = prepare(train_ds)\n",
    "\n",
    "val_ds = ds.skip(train_size)\n",
    "# val_ds = prepare(val_ds)\n",
    "\n",
    "# ds = prepare(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: (((None, 33, 4), (None, 468, 3), (None, 21, 3), (None, 21, 3)), (10,)), types: ((tf.float32, tf.float32, tf.float32, tf.float32), tf.float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 64\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL = 1\n",
    "log_dir = os.path.join('Heh/{}'.format(TRIAL))\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(FRAMES, 1662)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False, dropout=0.2)))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 64, 128)          884224    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64, 64)            8256      \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64, 128)          66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64, 64)            8256      \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 128)              66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,034,122\n",
      "Trainable params: 1,034,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 17s 614ms/step - loss: 2.2939 - accuracy: 0.0784 - val_loss: 2.2457 - val_accuracy: 0.1406\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 2.2519 - accuracy: 0.1176 - val_loss: 2.2501 - val_accuracy: 0.1406\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 2s 197ms/step - loss: 2.2309 - accuracy: 0.1490 - val_loss: 2.2304 - val_accuracy: 0.1875\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 2s 224ms/step - loss: 2.2160 - accuracy: 0.1490 - val_loss: 2.2143 - val_accuracy: 0.0938\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 2s 227ms/step - loss: 2.2125 - accuracy: 0.1490 - val_loss: 2.1384 - val_accuracy: 0.2188\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 2s 197ms/step - loss: 2.1834 - accuracy: 0.1529 - val_loss: 2.1715 - val_accuracy: 0.2344\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 2s 245ms/step - loss: 2.1802 - accuracy: 0.1569 - val_loss: 2.1495 - val_accuracy: 0.2031\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 2.1670 - accuracy: 0.1882 - val_loss: 2.1793 - val_accuracy: 0.1562\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 2.1805 - accuracy: 0.1451 - val_loss: 2.1669 - val_accuracy: 0.1875\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 2.1407 - accuracy: 0.1922 - val_loss: 2.1875 - val_accuracy: 0.1406\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 2s 204ms/step - loss: 2.1277 - accuracy: 0.1882 - val_loss: 2.0902 - val_accuracy: 0.2656\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 2.1221 - accuracy: 0.1843 - val_loss: 2.0511 - val_accuracy: 0.2188\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 2s 178ms/step - loss: 2.0926 - accuracy: 0.1686 - val_loss: 1.9948 - val_accuracy: 0.2344\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 2.0853 - accuracy: 0.2353 - val_loss: 2.0344 - val_accuracy: 0.3125\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 2s 192ms/step - loss: 2.0685 - accuracy: 0.1922 - val_loss: 2.0204 - val_accuracy: 0.2656\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 2.0547 - accuracy: 0.2392 - val_loss: 2.0635 - val_accuracy: 0.1719\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 2.0388 - accuracy: 0.2588 - val_loss: 1.9831 - val_accuracy: 0.3125\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 2.0383 - accuracy: 0.2431 - val_loss: 2.0308 - val_accuracy: 0.2656\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 2.0152 - accuracy: 0.2745 - val_loss: 2.0015 - val_accuracy: 0.2031\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 2.0094 - accuracy: 0.2353 - val_loss: 1.9675 - val_accuracy: 0.2812\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 2s 176ms/step - loss: 1.9782 - accuracy: 0.2784 - val_loss: 1.9711 - val_accuracy: 0.2812\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 2s 178ms/step - loss: 1.9461 - accuracy: 0.2863 - val_loss: 1.8926 - val_accuracy: 0.3281\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 2s 187ms/step - loss: 1.9419 - accuracy: 0.2745 - val_loss: 1.8837 - val_accuracy: 0.2969\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 1.9379 - accuracy: 0.2863 - val_loss: 1.8714 - val_accuracy: 0.3281\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 1.8886 - accuracy: 0.2784 - val_loss: 1.8515 - val_accuracy: 0.2812\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 1.8939 - accuracy: 0.2863 - val_loss: 1.8355 - val_accuracy: 0.2969\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 2s 174ms/step - loss: 1.9101 - accuracy: 0.3059 - val_loss: 1.9701 - val_accuracy: 0.1875\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 2s 169ms/step - loss: 1.8906 - accuracy: 0.2549 - val_loss: 1.8809 - val_accuracy: 0.2812\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 1.9509 - accuracy: 0.2706 - val_loss: 1.8449 - val_accuracy: 0.3438\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 2s 158ms/step - loss: 1.8705 - accuracy: 0.3216 - val_loss: 1.8081 - val_accuracy: 0.2812\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 2s 152ms/step - loss: 1.7925 - accuracy: 0.3569 - val_loss: 1.7786 - val_accuracy: 0.3906\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 2s 163ms/step - loss: 1.7690 - accuracy: 0.3647 - val_loss: 1.6958 - val_accuracy: 0.3906\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 2s 167ms/step - loss: 1.8158 - accuracy: 0.2980 - val_loss: 1.7015 - val_accuracy: 0.3438\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 2s 153ms/step - loss: 1.7320 - accuracy: 0.3922 - val_loss: 1.7411 - val_accuracy: 0.3906\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 2s 151ms/step - loss: 1.7424 - accuracy: 0.3412 - val_loss: 1.6971 - val_accuracy: 0.4062\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 2s 154ms/step - loss: 1.7151 - accuracy: 0.3804 - val_loss: 1.6512 - val_accuracy: 0.3594\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 2s 158ms/step - loss: 1.7717 - accuracy: 0.3647 - val_loss: 1.8094 - val_accuracy: 0.2812\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 2s 154ms/step - loss: 1.8081 - accuracy: 0.3137 - val_loss: 1.6972 - val_accuracy: 0.2656\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 1.6923 - accuracy: 0.3882 - val_loss: 1.7417 - val_accuracy: 0.3438\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 2s 169ms/step - loss: 1.6626 - accuracy: 0.3765 - val_loss: 1.6156 - val_accuracy: 0.4062\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 2s 166ms/step - loss: 1.6700 - accuracy: 0.3725 - val_loss: 1.6456 - val_accuracy: 0.3594\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 2s 158ms/step - loss: 1.6123 - accuracy: 0.4157 - val_loss: 1.5233 - val_accuracy: 0.4062\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 2s 170ms/step - loss: 1.6685 - accuracy: 0.3725 - val_loss: 1.6463 - val_accuracy: 0.4219\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 2s 187ms/step - loss: 1.6634 - accuracy: 0.4039 - val_loss: 1.7520 - val_accuracy: 0.2812\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 2s 197ms/step - loss: 1.6519 - accuracy: 0.4078 - val_loss: 1.5983 - val_accuracy: 0.4219\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 2s 179ms/step - loss: 1.5323 - accuracy: 0.4431 - val_loss: 1.4199 - val_accuracy: 0.5781\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 1.5365 - accuracy: 0.4667 - val_loss: 1.5284 - val_accuracy: 0.5156\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 2s 174ms/step - loss: 1.4789 - accuracy: 0.4902 - val_loss: 1.4136 - val_accuracy: 0.4688\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 2s 168ms/step - loss: 1.4918 - accuracy: 0.4510 - val_loss: 1.3243 - val_accuracy: 0.5938\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 2s 171ms/step - loss: 1.4835 - accuracy: 0.4941 - val_loss: 1.4245 - val_accuracy: 0.5469\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 2s 172ms/step - loss: 1.4712 - accuracy: 0.4824 - val_loss: 1.5654 - val_accuracy: 0.3750\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 2s 171ms/step - loss: 1.4759 - accuracy: 0.4549 - val_loss: 1.4974 - val_accuracy: 0.5000\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 1.4557 - accuracy: 0.5137 - val_loss: 1.5041 - val_accuracy: 0.3906\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 2s 170ms/step - loss: 1.5498 - accuracy: 0.4549 - val_loss: 1.6664 - val_accuracy: 0.3281\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 2s 193ms/step - loss: 1.5112 - accuracy: 0.4314 - val_loss: 1.3887 - val_accuracy: 0.4375\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 2s 229ms/step - loss: 1.4830 - accuracy: 0.4784 - val_loss: 1.4163 - val_accuracy: 0.4531\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 1.3754 - accuracy: 0.4824 - val_loss: 1.6547 - val_accuracy: 0.4375\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 1.4318 - accuracy: 0.4745 - val_loss: 1.2978 - val_accuracy: 0.5000\n",
      "Epoch 59/2000\n",
      "8/8 [==============================] - 2s 195ms/step - loss: 1.3920 - accuracy: 0.4863 - val_loss: 1.3115 - val_accuracy: 0.5312\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 1.3769 - accuracy: 0.4784 - val_loss: 1.4129 - val_accuracy: 0.5156\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 2s 196ms/step - loss: 1.3261 - accuracy: 0.5333 - val_loss: 1.3624 - val_accuracy: 0.5781\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 1.3828 - accuracy: 0.5216 - val_loss: 1.3445 - val_accuracy: 0.4844\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 2s 220ms/step - loss: 1.3251 - accuracy: 0.5294 - val_loss: 1.2570 - val_accuracy: 0.6094\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 1.3094 - accuracy: 0.5412 - val_loss: 1.1939 - val_accuracy: 0.5625\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 2s 234ms/step - loss: 1.5175 - accuracy: 0.4235 - val_loss: 1.4469 - val_accuracy: 0.4688\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 1.4812 - accuracy: 0.4549 - val_loss: 1.4134 - val_accuracy: 0.4531\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 2s 195ms/step - loss: 1.3771 - accuracy: 0.4784 - val_loss: 1.3973 - val_accuracy: 0.5000\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 1.3034 - accuracy: 0.5647 - val_loss: 1.1313 - val_accuracy: 0.5938\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 2s 179ms/step - loss: 1.3185 - accuracy: 0.5255 - val_loss: 1.3687 - val_accuracy: 0.5469\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 2s 178ms/step - loss: 1.2827 - accuracy: 0.5216 - val_loss: 1.3245 - val_accuracy: 0.5156\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 1.3075 - accuracy: 0.5608 - val_loss: 1.2749 - val_accuracy: 0.5469\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 1.1919 - accuracy: 0.5686 - val_loss: 1.1060 - val_accuracy: 0.5938\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 2s 197ms/step - loss: 1.1987 - accuracy: 0.5961 - val_loss: 1.1479 - val_accuracy: 0.5469\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 2s 191ms/step - loss: 1.2829 - accuracy: 0.5490 - val_loss: 1.4847 - val_accuracy: 0.4375\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 1.3741 - accuracy: 0.5333 - val_loss: 1.5349 - val_accuracy: 0.4531\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 2s 191ms/step - loss: 1.4630 - accuracy: 0.4431 - val_loss: 1.3159 - val_accuracy: 0.4844\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 2s 239ms/step - loss: 1.3361 - accuracy: 0.4902 - val_loss: 1.4005 - val_accuracy: 0.5000\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 2s 170ms/step - loss: 1.3823 - accuracy: 0.4392 - val_loss: 1.2393 - val_accuracy: 0.5625\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 2s 157ms/step - loss: 1.3116 - accuracy: 0.5412 - val_loss: 1.3604 - val_accuracy: 0.4062\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 2s 176ms/step - loss: 1.2179 - accuracy: 0.5373 - val_loss: 1.2027 - val_accuracy: 0.5938\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 2s 165ms/step - loss: 1.2176 - accuracy: 0.5725 - val_loss: 1.1915 - val_accuracy: 0.5781\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 1.0983 - accuracy: 0.5765 - val_loss: 0.9594 - val_accuracy: 0.7344\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 2s 159ms/step - loss: 1.1004 - accuracy: 0.6510 - val_loss: 1.0190 - val_accuracy: 0.6875\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 2s 162ms/step - loss: 1.1596 - accuracy: 0.6078 - val_loss: 1.0745 - val_accuracy: 0.6562\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 2s 162ms/step - loss: 1.1214 - accuracy: 0.5765 - val_loss: 1.0244 - val_accuracy: 0.6406\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 2s 159ms/step - loss: 1.0556 - accuracy: 0.6431 - val_loss: 1.3615 - val_accuracy: 0.5000\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 1.1117 - accuracy: 0.6275 - val_loss: 1.0228 - val_accuracy: 0.6562\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 2s 161ms/step - loss: 1.1820 - accuracy: 0.5725 - val_loss: 1.2007 - val_accuracy: 0.5469\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 2s 234ms/step - loss: 1.0979 - accuracy: 0.5961 - val_loss: 1.1647 - val_accuracy: 0.5781\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 1.1012 - accuracy: 0.6157 - val_loss: 0.9894 - val_accuracy: 0.6719\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 2s 178ms/step - loss: 1.1279 - accuracy: 0.6118 - val_loss: 0.9982 - val_accuracy: 0.6406\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 2s 178ms/step - loss: 1.1090 - accuracy: 0.5725 - val_loss: 0.9701 - val_accuracy: 0.5938\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 2s 176ms/step - loss: 1.1826 - accuracy: 0.5333 - val_loss: 1.3616 - val_accuracy: 0.5312\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 1.3281 - accuracy: 0.5333 - val_loss: 1.3485 - val_accuracy: 0.5938\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 2s 165ms/step - loss: 1.3708 - accuracy: 0.5098 - val_loss: 1.0689 - val_accuracy: 0.6250\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 2s 171ms/step - loss: 1.1674 - accuracy: 0.6000 - val_loss: 1.2412 - val_accuracy: 0.5938\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 2s 215ms/step - loss: 1.0367 - accuracy: 0.6235 - val_loss: 0.9163 - val_accuracy: 0.7344\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 1.0059 - accuracy: 0.6588 - val_loss: 1.1773 - val_accuracy: 0.5625\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 2s 222ms/step - loss: 1.0444 - accuracy: 0.6118 - val_loss: 0.9859 - val_accuracy: 0.6250\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 2s 225ms/step - loss: 1.0457 - accuracy: 0.6118 - val_loss: 1.0015 - val_accuracy: 0.7031\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 1.0628 - accuracy: 0.6235 - val_loss: 1.1918 - val_accuracy: 0.5938\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 2s 211ms/step - loss: 1.0141 - accuracy: 0.6078 - val_loss: 0.9244 - val_accuracy: 0.6406\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 2s 204ms/step - loss: 1.0837 - accuracy: 0.6196 - val_loss: 0.9838 - val_accuracy: 0.6719\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 2s 217ms/step - loss: 1.0182 - accuracy: 0.6510 - val_loss: 1.0144 - val_accuracy: 0.6406\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 1.0609 - accuracy: 0.6000 - val_loss: 1.0774 - val_accuracy: 0.5625\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 1.0324 - accuracy: 0.6392 - val_loss: 0.9984 - val_accuracy: 0.6875\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 2s 213ms/step - loss: 0.9563 - accuracy: 0.6863 - val_loss: 0.7710 - val_accuracy: 0.7812\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 1.0099 - accuracy: 0.6353 - val_loss: 0.7881 - val_accuracy: 0.7969\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 2s 175ms/step - loss: 1.1015 - accuracy: 0.5843 - val_loss: 1.0305 - val_accuracy: 0.6250\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 1.0397 - accuracy: 0.6392 - val_loss: 1.2583 - val_accuracy: 0.5000\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 1.0376 - accuracy: 0.6000 - val_loss: 1.1481 - val_accuracy: 0.5938\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 2s 195ms/step - loss: 0.9926 - accuracy: 0.6392 - val_loss: 1.1111 - val_accuracy: 0.5000\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 1.0796 - accuracy: 0.6078 - val_loss: 0.9188 - val_accuracy: 0.6719\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 2s 193ms/step - loss: 1.1209 - accuracy: 0.5804 - val_loss: 1.0488 - val_accuracy: 0.6562\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 2s 179ms/step - loss: 1.1175 - accuracy: 0.5961 - val_loss: 0.9082 - val_accuracy: 0.6719\n",
      "Epoch 116/2000\n",
      "8/8 [==============================] - 2s 186ms/step - loss: 0.9690 - accuracy: 0.6784 - val_loss: 0.8093 - val_accuracy: 0.6719\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.8509 - accuracy: 0.7255 - val_loss: 0.8900 - val_accuracy: 0.7188\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 0.9029 - accuracy: 0.6824 - val_loss: 0.7685 - val_accuracy: 0.7812\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 2s 192ms/step - loss: 0.8996 - accuracy: 0.7059 - val_loss: 0.8003 - val_accuracy: 0.6875\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.8837 - accuracy: 0.7020 - val_loss: 0.8759 - val_accuracy: 0.6719\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 2s 171ms/step - loss: 0.8317 - accuracy: 0.7373 - val_loss: 0.7971 - val_accuracy: 0.7656\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 2s 175ms/step - loss: 0.8056 - accuracy: 0.7294 - val_loss: 0.8719 - val_accuracy: 0.6875\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 0.8662 - accuracy: 0.7176 - val_loss: 0.7369 - val_accuracy: 0.7812\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.8123 - accuracy: 0.7255 - val_loss: 0.7654 - val_accuracy: 0.7344\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 2s 186ms/step - loss: 0.7745 - accuracy: 0.7451 - val_loss: 0.5758 - val_accuracy: 0.7812\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 0.8743 - accuracy: 0.6902 - val_loss: 0.8139 - val_accuracy: 0.6875\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 2s 191ms/step - loss: 0.9087 - accuracy: 0.6902 - val_loss: 0.6936 - val_accuracy: 0.7812\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 0.8144 - accuracy: 0.7098 - val_loss: 0.6149 - val_accuracy: 0.8125\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 2s 192ms/step - loss: 0.8059 - accuracy: 0.7176 - val_loss: 0.6893 - val_accuracy: 0.7656\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.8247 - accuracy: 0.7373 - val_loss: 0.7357 - val_accuracy: 0.7344\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.7766 - accuracy: 0.7373 - val_loss: 0.7179 - val_accuracy: 0.8125\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 3s 291ms/step - loss: 0.8382 - accuracy: 0.6784 - val_loss: 0.8306 - val_accuracy: 0.6875\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.9090 - accuracy: 0.6667 - val_loss: 0.8820 - val_accuracy: 0.6719\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 2s 174ms/step - loss: 1.0040 - accuracy: 0.6824 - val_loss: 0.8126 - val_accuracy: 0.6719\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 2s 176ms/step - loss: 1.0221 - accuracy: 0.6314 - val_loss: 0.9670 - val_accuracy: 0.6719\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.8694 - accuracy: 0.7098 - val_loss: 0.8602 - val_accuracy: 0.6562\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.8874 - accuracy: 0.6980 - val_loss: 0.8638 - val_accuracy: 0.7344\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.8739 - accuracy: 0.6745 - val_loss: 0.7684 - val_accuracy: 0.7656\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 0.7392 - accuracy: 0.7529 - val_loss: 0.7354 - val_accuracy: 0.7344\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 0.7258 - accuracy: 0.7608 - val_loss: 0.8610 - val_accuracy: 0.7031\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 2s 169ms/step - loss: 0.7044 - accuracy: 0.7529 - val_loss: 0.7417 - val_accuracy: 0.7812\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 2s 191ms/step - loss: 0.7156 - accuracy: 0.7843 - val_loss: 0.8455 - val_accuracy: 0.7188\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.8533 - accuracy: 0.7059 - val_loss: 0.9573 - val_accuracy: 0.6250\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 2s 183ms/step - loss: 0.7190 - accuracy: 0.7765 - val_loss: 0.6985 - val_accuracy: 0.7969\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 2s 187ms/step - loss: 0.7727 - accuracy: 0.7216 - val_loss: 0.6852 - val_accuracy: 0.7656\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 2s 169ms/step - loss: 0.7346 - accuracy: 0.7451 - val_loss: 0.7989 - val_accuracy: 0.7656\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 2s 166ms/step - loss: 0.7900 - accuracy: 0.7137 - val_loss: 0.6437 - val_accuracy: 0.7969\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 2s 167ms/step - loss: 0.6694 - accuracy: 0.7843 - val_loss: 0.8118 - val_accuracy: 0.7500\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 0.8398 - accuracy: 0.6667 - val_loss: 0.8662 - val_accuracy: 0.6406\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 1.0314 - accuracy: 0.6157 - val_loss: 0.9642 - val_accuracy: 0.5938\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.8541 - accuracy: 0.6863 - val_loss: 0.7212 - val_accuracy: 0.7656\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 0.8719 - accuracy: 0.6588 - val_loss: 0.6330 - val_accuracy: 0.7812\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.7029 - accuracy: 0.7804 - val_loss: 0.9159 - val_accuracy: 0.6562\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.8116 - accuracy: 0.6941 - val_loss: 0.6763 - val_accuracy: 0.7812\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 2s 186ms/step - loss: 0.7262 - accuracy: 0.7529 - val_loss: 0.5630 - val_accuracy: 0.7656\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 0.6620 - accuracy: 0.7765 - val_loss: 0.6442 - val_accuracy: 0.8594\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.5871 - accuracy: 0.8431 - val_loss: 0.5979 - val_accuracy: 0.7812\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.5963 - accuracy: 0.8000 - val_loss: 0.6465 - val_accuracy: 0.8125\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 0.6103 - accuracy: 0.8000 - val_loss: 0.4329 - val_accuracy: 0.9219\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 2s 165ms/step - loss: 0.6882 - accuracy: 0.7451 - val_loss: 0.7054 - val_accuracy: 0.7656\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 2s 174ms/step - loss: 0.8004 - accuracy: 0.7451 - val_loss: 0.6099 - val_accuracy: 0.7656\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 0.6671 - accuracy: 0.7647 - val_loss: 0.5034 - val_accuracy: 0.8281\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 0.6991 - accuracy: 0.7647 - val_loss: 0.6732 - val_accuracy: 0.7656\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.5755 - accuracy: 0.8118 - val_loss: 0.6065 - val_accuracy: 0.7812\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 2s 183ms/step - loss: 0.7310 - accuracy: 0.7569 - val_loss: 0.5947 - val_accuracy: 0.7969\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 2s 187ms/step - loss: 0.7361 - accuracy: 0.7529 - val_loss: 0.6094 - val_accuracy: 0.7812\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 0.7395 - accuracy: 0.7608 - val_loss: 0.5965 - val_accuracy: 0.8281\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.6611 - accuracy: 0.7961 - val_loss: 0.7041 - val_accuracy: 0.7344\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.7119 - accuracy: 0.7412 - val_loss: 0.6336 - val_accuracy: 0.7969\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.6297 - accuracy: 0.8039 - val_loss: 0.5753 - val_accuracy: 0.8125\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 2s 176ms/step - loss: 0.6133 - accuracy: 0.7804 - val_loss: 0.5617 - val_accuracy: 0.7969\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 2s 186ms/step - loss: 0.7018 - accuracy: 0.7725 - val_loss: 0.5745 - val_accuracy: 0.8125\n",
      "Epoch 173/2000\n",
      "8/8 [==============================] - 2s 210ms/step - loss: 0.6871 - accuracy: 0.7569 - val_loss: 0.7059 - val_accuracy: 0.7344\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.6650 - accuracy: 0.7922 - val_loss: 0.6269 - val_accuracy: 0.7812\n",
      "Epoch 175/2000\n",
      "8/8 [==============================] - 2s 187ms/step - loss: 0.5893 - accuracy: 0.8118 - val_loss: 0.5195 - val_accuracy: 0.8750\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.5267 - accuracy: 0.8431 - val_loss: 0.4796 - val_accuracy: 0.8594\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 0.5114 - accuracy: 0.8392 - val_loss: 0.4357 - val_accuracy: 0.8750\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.5630 - accuracy: 0.8078 - val_loss: 0.6058 - val_accuracy: 0.7812\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.5704 - accuracy: 0.8157 - val_loss: 0.7147 - val_accuracy: 0.7031\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 0.5554 - accuracy: 0.8196 - val_loss: 0.5639 - val_accuracy: 0.7656\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.5776 - accuracy: 0.8196 - val_loss: 0.5323 - val_accuracy: 0.7812\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 0.5814 - accuracy: 0.8039 - val_loss: 0.4191 - val_accuracy: 0.8594\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 2s 169ms/step - loss: 0.5167 - accuracy: 0.8314 - val_loss: 0.3838 - val_accuracy: 0.8750\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 0.5447 - accuracy: 0.8196 - val_loss: 0.5819 - val_accuracy: 0.7500\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 2s 216ms/step - loss: 0.5051 - accuracy: 0.8431 - val_loss: 0.6660 - val_accuracy: 0.7812\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 2s 210ms/step - loss: 0.5197 - accuracy: 0.8392 - val_loss: 0.4700 - val_accuracy: 0.8594\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 2s 236ms/step - loss: 0.6662 - accuracy: 0.7647 - val_loss: 0.4314 - val_accuracy: 0.9062\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 2s 196ms/step - loss: 0.5643 - accuracy: 0.8118 - val_loss: 0.6318 - val_accuracy: 0.8438\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.5181 - accuracy: 0.8353 - val_loss: 0.6364 - val_accuracy: 0.8125\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.6086 - accuracy: 0.8314 - val_loss: 0.4437 - val_accuracy: 0.8594\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.6109 - accuracy: 0.8039 - val_loss: 0.5151 - val_accuracy: 0.8594\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 2s 183ms/step - loss: 0.5140 - accuracy: 0.8078 - val_loss: 0.5833 - val_accuracy: 0.7969\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.5952 - accuracy: 0.7882 - val_loss: 0.4745 - val_accuracy: 0.8594\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.5758 - accuracy: 0.8078 - val_loss: 0.5592 - val_accuracy: 0.8594\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 2s 171ms/step - loss: 0.7595 - accuracy: 0.7608 - val_loss: 0.7289 - val_accuracy: 0.7656\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 2s 186ms/step - loss: 0.6956 - accuracy: 0.7843 - val_loss: 0.5374 - val_accuracy: 0.8281\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 2s 174ms/step - loss: 0.6407 - accuracy: 0.7608 - val_loss: 0.4447 - val_accuracy: 0.8906\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.5316 - accuracy: 0.8314 - val_loss: 0.5082 - val_accuracy: 0.8281\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.8668 - accuracy: 0.6980 - val_loss: 0.9367 - val_accuracy: 0.6562\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 1.0422 - accuracy: 0.6039 - val_loss: 0.8339 - val_accuracy: 0.7031\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 2s 179ms/step - loss: 0.8893 - accuracy: 0.6588 - val_loss: 0.9333 - val_accuracy: 0.7188\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.8948 - accuracy: 0.7137 - val_loss: 0.7431 - val_accuracy: 0.7344\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 2s 183ms/step - loss: 0.8857 - accuracy: 0.6941 - val_loss: 0.7261 - val_accuracy: 0.6875\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 2s 187ms/step - loss: 0.8018 - accuracy: 0.7098 - val_loss: 0.7506 - val_accuracy: 0.7188\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.7178 - accuracy: 0.7333 - val_loss: 0.7312 - val_accuracy: 0.6719\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 2s 192ms/step - loss: 0.7887 - accuracy: 0.7137 - val_loss: 0.8654 - val_accuracy: 0.6719\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 2s 174ms/step - loss: 0.7281 - accuracy: 0.7647 - val_loss: 0.6281 - val_accuracy: 0.8125\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 2s 171ms/step - loss: 0.6020 - accuracy: 0.8353 - val_loss: 0.6671 - val_accuracy: 0.7500\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.5928 - accuracy: 0.8157 - val_loss: 0.3987 - val_accuracy: 0.9219\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 0.6159 - accuracy: 0.8000 - val_loss: 0.7756 - val_accuracy: 0.7500\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.8761 - accuracy: 0.6706 - val_loss: 0.7090 - val_accuracy: 0.7500\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 2s 183ms/step - loss: 0.7396 - accuracy: 0.7490 - val_loss: 0.7559 - val_accuracy: 0.7188\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 2s 178ms/step - loss: 0.5936 - accuracy: 0.8000 - val_loss: 0.5634 - val_accuracy: 0.8594\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 2s 183ms/step - loss: 0.5495 - accuracy: 0.8157 - val_loss: 0.6144 - val_accuracy: 0.7969\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 2s 178ms/step - loss: 0.5205 - accuracy: 0.8078 - val_loss: 0.5403 - val_accuracy: 0.7812\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.4982 - accuracy: 0.8431 - val_loss: 0.4725 - val_accuracy: 0.8906\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.4846 - accuracy: 0.8431 - val_loss: 0.5582 - val_accuracy: 0.8438\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 0.4804 - accuracy: 0.8627 - val_loss: 0.4397 - val_accuracy: 0.8594\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 0.4166 - accuracy: 0.8549 - val_loss: 0.3975 - val_accuracy: 0.8594\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 2s 166ms/step - loss: 0.4584 - accuracy: 0.8549 - val_loss: 0.5351 - val_accuracy: 0.7969\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 2s 178ms/step - loss: 0.4711 - accuracy: 0.8235 - val_loss: 0.4767 - val_accuracy: 0.8594\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 2s 206ms/step - loss: 0.5146 - accuracy: 0.8471 - val_loss: 0.5624 - val_accuracy: 0.8438\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 2s 195ms/step - loss: 0.5107 - accuracy: 0.8314 - val_loss: 0.6680 - val_accuracy: 0.7656\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 0.5320 - accuracy: 0.8157 - val_loss: 0.5010 - val_accuracy: 0.8281\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 2s 197ms/step - loss: 0.5018 - accuracy: 0.8549 - val_loss: 0.5052 - val_accuracy: 0.8438\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.6026 - accuracy: 0.7608 - val_loss: 0.4417 - val_accuracy: 0.8281\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 2s 215ms/step - loss: 0.4613 - accuracy: 0.8627 - val_loss: 0.3452 - val_accuracy: 0.8750\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.4800 - accuracy: 0.8275 - val_loss: 0.4512 - val_accuracy: 0.8438\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 2s 201ms/step - loss: 0.3791 - accuracy: 0.8980 - val_loss: 0.3280 - val_accuracy: 0.9062\n",
      "Epoch 230/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.3905 - accuracy: 0.8980 - val_loss: 0.3032 - val_accuracy: 0.9219\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 2s 194ms/step - loss: 0.3907 - accuracy: 0.8784 - val_loss: 0.2654 - val_accuracy: 0.9375\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 2s 242ms/step - loss: 0.3734 - accuracy: 0.8824 - val_loss: 0.3873 - val_accuracy: 0.8594\n",
      "Epoch 233/2000\n",
      "8/8 [==============================] - 2s 214ms/step - loss: 0.3346 - accuracy: 0.9137 - val_loss: 0.4175 - val_accuracy: 0.8750\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 2s 226ms/step - loss: 0.3375 - accuracy: 0.8941 - val_loss: 0.3550 - val_accuracy: 0.8750\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 2s 226ms/step - loss: 0.3715 - accuracy: 0.9059 - val_loss: 0.5195 - val_accuracy: 0.8281\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 2s 199ms/step - loss: 0.4539 - accuracy: 0.8392 - val_loss: 0.4082 - val_accuracy: 0.8438\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 2s 193ms/step - loss: 0.4095 - accuracy: 0.8667 - val_loss: 0.3251 - val_accuracy: 0.8906\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.3974 - accuracy: 0.8667 - val_loss: 0.3540 - val_accuracy: 0.9375\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 0.3559 - accuracy: 0.8784 - val_loss: 0.4032 - val_accuracy: 0.8594\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 0.4001 - accuracy: 0.8824 - val_loss: 0.4104 - val_accuracy: 0.8750\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.3831 - accuracy: 0.8627 - val_loss: 0.3526 - val_accuracy: 0.8906\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.4021 - accuracy: 0.8745 - val_loss: 0.4216 - val_accuracy: 0.8438\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.4940 - accuracy: 0.8431 - val_loss: 0.3005 - val_accuracy: 0.9375\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 2s 170ms/step - loss: 0.3890 - accuracy: 0.8784 - val_loss: 0.2896 - val_accuracy: 0.9375\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 2s 170ms/step - loss: 0.3999 - accuracy: 0.8667 - val_loss: 0.2651 - val_accuracy: 0.9375\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 0.3902 - accuracy: 0.8824 - val_loss: 0.4279 - val_accuracy: 0.8750\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 0.4460 - accuracy: 0.8549 - val_loss: 0.5523 - val_accuracy: 0.8281\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.6174 - accuracy: 0.7882 - val_loss: 0.5717 - val_accuracy: 0.7656\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 2s 192ms/step - loss: 0.7139 - accuracy: 0.7765 - val_loss: 0.7665 - val_accuracy: 0.7500\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 2s 186ms/step - loss: 0.6885 - accuracy: 0.7647 - val_loss: 0.5493 - val_accuracy: 0.7969\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.6789 - accuracy: 0.7686 - val_loss: 0.5641 - val_accuracy: 0.8594\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 2s 191ms/step - loss: 0.7838 - accuracy: 0.7098 - val_loss: 0.7229 - val_accuracy: 0.7500\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 0.6897 - accuracy: 0.7608 - val_loss: 0.7094 - val_accuracy: 0.8281\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.5687 - accuracy: 0.8039 - val_loss: 0.4936 - val_accuracy: 0.8438\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.5352 - accuracy: 0.8235 - val_loss: 0.3703 - val_accuracy: 0.8906\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 2s 176ms/step - loss: 0.4729 - accuracy: 0.8392 - val_loss: 0.4498 - val_accuracy: 0.8750\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 2s 175ms/step - loss: 0.4975 - accuracy: 0.8275 - val_loss: 0.4631 - val_accuracy: 0.8281\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 2s 169ms/step - loss: 0.3430 - accuracy: 0.8824 - val_loss: 0.3986 - val_accuracy: 0.8750\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.4294 - accuracy: 0.8431 - val_loss: 0.4544 - val_accuracy: 0.8438\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 2s 187ms/step - loss: 0.3854 - accuracy: 0.8863 - val_loss: 0.2833 - val_accuracy: 0.9062\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 2s 196ms/step - loss: 0.3235 - accuracy: 0.8980 - val_loss: 0.2441 - val_accuracy: 0.9375\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.3377 - accuracy: 0.9137 - val_loss: 0.2104 - val_accuracy: 0.9688\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 2s 196ms/step - loss: 0.3289 - accuracy: 0.9059 - val_loss: 0.2060 - val_accuracy: 0.9688\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 2s 193ms/step - loss: 0.3172 - accuracy: 0.9098 - val_loss: 0.2946 - val_accuracy: 0.8906\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.2688 - accuracy: 0.9294 - val_loss: 0.3639 - val_accuracy: 0.8750\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.3045 - accuracy: 0.9176 - val_loss: 0.2376 - val_accuracy: 0.9219\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 2s 193ms/step - loss: 0.3812 - accuracy: 0.8627 - val_loss: 0.3131 - val_accuracy: 0.9219\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 0.3343 - accuracy: 0.9176 - val_loss: 0.2797 - val_accuracy: 0.9219\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 2s 234ms/step - loss: 0.3373 - accuracy: 0.8824 - val_loss: 0.3256 - val_accuracy: 0.9062\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 2s 175ms/step - loss: 0.2898 - accuracy: 0.9176 - val_loss: 0.3143 - val_accuracy: 0.9531\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 2s 228ms/step - loss: 0.3414 - accuracy: 0.9059 - val_loss: 0.2005 - val_accuracy: 0.9844\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 2s 272ms/step - loss: 0.2628 - accuracy: 0.9294 - val_loss: 0.3493 - val_accuracy: 0.8438\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 2s 228ms/step - loss: 0.2719 - accuracy: 0.9176 - val_loss: 0.2735 - val_accuracy: 0.9219\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.2869 - accuracy: 0.9137 - val_loss: 0.3560 - val_accuracy: 0.8906\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 2s 193ms/step - loss: 0.3002 - accuracy: 0.9176 - val_loss: 0.2475 - val_accuracy: 0.9219\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.3514 - accuracy: 0.8784 - val_loss: 0.3046 - val_accuracy: 0.9219\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 2s 196ms/step - loss: 0.4591 - accuracy: 0.8275 - val_loss: 0.1551 - val_accuracy: 0.9844\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 2s 195ms/step - loss: 0.4897 - accuracy: 0.8235 - val_loss: 0.4119 - val_accuracy: 0.8750\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 2s 217ms/step - loss: 0.5721 - accuracy: 0.8039 - val_loss: 0.2996 - val_accuracy: 0.9531\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 2s 217ms/step - loss: 0.4931 - accuracy: 0.8078 - val_loss: 0.3143 - val_accuracy: 0.9219\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 2s 226ms/step - loss: 0.5543 - accuracy: 0.8235 - val_loss: 0.4963 - val_accuracy: 0.7969\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 2s 179ms/step - loss: 0.5880 - accuracy: 0.7765 - val_loss: 0.6071 - val_accuracy: 0.7656\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 2s 178ms/step - loss: 0.5214 - accuracy: 0.8510 - val_loss: 0.6689 - val_accuracy: 0.7656\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 2s 156ms/step - loss: 0.6250 - accuracy: 0.7765 - val_loss: 0.5338 - val_accuracy: 0.8125\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.5947 - accuracy: 0.7843 - val_loss: 0.3975 - val_accuracy: 0.8438\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 0.4702 - accuracy: 0.8392 - val_loss: 0.4333 - val_accuracy: 0.8281\n",
      "Epoch 287/2000\n",
      "8/8 [==============================] - 2s 211ms/step - loss: 0.4083 - accuracy: 0.8627 - val_loss: 0.3986 - val_accuracy: 0.8750\n",
      "Epoch 288/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 0.4105 - accuracy: 0.8510 - val_loss: 0.5205 - val_accuracy: 0.7969\n",
      "Epoch 289/2000\n",
      "8/8 [==============================] - 2s 169ms/step - loss: 0.4624 - accuracy: 0.8588 - val_loss: 0.4258 - val_accuracy: 0.8438\n",
      "Epoch 290/2000\n",
      "8/8 [==============================] - 2s 172ms/step - loss: 0.4226 - accuracy: 0.8392 - val_loss: 0.2911 - val_accuracy: 0.8906\n",
      "Epoch 291/2000\n",
      "8/8 [==============================] - 2s 195ms/step - loss: 0.3796 - accuracy: 0.8627 - val_loss: 0.2663 - val_accuracy: 0.9375\n",
      "Epoch 292/2000\n",
      "8/8 [==============================] - 2s 169ms/step - loss: 0.3544 - accuracy: 0.8902 - val_loss: 0.2905 - val_accuracy: 0.9531\n",
      "Epoch 293/2000\n",
      "8/8 [==============================] - 2s 169ms/step - loss: 0.3031 - accuracy: 0.9137 - val_loss: 0.3390 - val_accuracy: 0.8750\n",
      "Epoch 294/2000\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 0.3727 - accuracy: 0.8824 - val_loss: 0.2884 - val_accuracy: 0.8906\n",
      "Epoch 295/2000\n",
      "8/8 [==============================] - 2s 170ms/step - loss: 0.2812 - accuracy: 0.9255 - val_loss: 0.2902 - val_accuracy: 0.9375\n",
      "Epoch 296/2000\n",
      "8/8 [==============================] - 2s 158ms/step - loss: 0.2761 - accuracy: 0.9255 - val_loss: 0.2045 - val_accuracy: 0.9688\n",
      "Epoch 297/2000\n",
      "8/8 [==============================] - 2s 156ms/step - loss: 0.2710 - accuracy: 0.9294 - val_loss: 0.2190 - val_accuracy: 0.9375\n",
      "Epoch 298/2000\n",
      "8/8 [==============================] - 2s 169ms/step - loss: 0.3398 - accuracy: 0.8941 - val_loss: 0.3044 - val_accuracy: 0.9062\n",
      "Epoch 299/2000\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 0.3252 - accuracy: 0.9216 - val_loss: 0.2984 - val_accuracy: 0.9062\n",
      "Epoch 300/2000\n",
      "8/8 [==============================] - 2s 171ms/step - loss: 0.3087 - accuracy: 0.9020 - val_loss: 0.1982 - val_accuracy: 0.9531\n",
      "Epoch 301/2000\n",
      "8/8 [==============================] - 2s 213ms/step - loss: 0.2961 - accuracy: 0.9059 - val_loss: 0.1903 - val_accuracy: 0.9688\n",
      "Epoch 302/2000\n",
      "8/8 [==============================] - 2s 195ms/step - loss: 0.3176 - accuracy: 0.9098 - val_loss: 0.2251 - val_accuracy: 0.9531\n",
      "Epoch 303/2000\n",
      "8/8 [==============================] - 2s 223ms/step - loss: 0.2802 - accuracy: 0.9333 - val_loss: 0.4249 - val_accuracy: 0.8594\n",
      "Epoch 304/2000\n",
      "8/8 [==============================] - 2s 176ms/step - loss: 0.2928 - accuracy: 0.9176 - val_loss: 0.4603 - val_accuracy: 0.8594\n",
      "Epoch 305/2000\n",
      "8/8 [==============================] - 2s 169ms/step - loss: 0.3401 - accuracy: 0.8902 - val_loss: 0.2427 - val_accuracy: 0.9375\n",
      "Epoch 306/2000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.2955 - accuracy: 0.8980 - val_loss: 0.2709 - val_accuracy: 0.9375\n",
      "Epoch 307/2000\n",
      "8/8 [==============================] - 2s 192ms/step - loss: 0.2933 - accuracy: 0.8980 - val_loss: 0.3916 - val_accuracy: 0.8906\n",
      "Epoch 308/2000\n",
      "8/8 [==============================] - 2s 164ms/step - loss: 0.3034 - accuracy: 0.9176 - val_loss: 0.2526 - val_accuracy: 0.9375\n",
      "Epoch 309/2000\n",
      "8/8 [==============================] - 2s 160ms/step - loss: 0.4260 - accuracy: 0.8824 - val_loss: 1.5139 - val_accuracy: 0.5469\n",
      "Epoch 310/2000\n",
      "8/8 [==============================] - 2s 167ms/step - loss: 0.7110 - accuracy: 0.7882 - val_loss: 0.5643 - val_accuracy: 0.8594\n",
      "Epoch 311/2000\n",
      "8/8 [==============================] - 2s 178ms/step - loss: 0.4797 - accuracy: 0.8275 - val_loss: 0.4700 - val_accuracy: 0.8750\n",
      "Epoch 312/2000\n",
      "8/8 [==============================] - 2s 167ms/step - loss: 0.5367 - accuracy: 0.8118 - val_loss: 0.4103 - val_accuracy: 0.8125\n",
      "Epoch 313/2000\n",
      "8/8 [==============================] - 2s 196ms/step - loss: 0.5097 - accuracy: 0.8078 - val_loss: 0.2434 - val_accuracy: 0.9531\n",
      "Epoch 314/2000\n",
      "8/8 [==============================] - 2s 176ms/step - loss: 0.4186 - accuracy: 0.8667 - val_loss: 0.6629 - val_accuracy: 0.7812\n",
      "Epoch 315/2000\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 0.5074 - accuracy: 0.8078 - val_loss: 0.2302 - val_accuracy: 0.9531\n",
      "Epoch 316/2000\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 0.3962 - accuracy: 0.8784 - val_loss: 0.2983 - val_accuracy: 0.9219\n",
      "Epoch 317/2000\n",
      "8/8 [==============================] - 2s 169ms/step - loss: 0.3692 - accuracy: 0.8627 - val_loss: 0.5055 - val_accuracy: 0.8438\n",
      "Epoch 318/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.4206 - accuracy: 0.8549 - val_loss: 0.4525 - val_accuracy: 0.8750\n",
      "Epoch 319/2000\n",
      "8/8 [==============================] - 2s 167ms/step - loss: 0.4366 - accuracy: 0.8510 - val_loss: 0.3656 - val_accuracy: 0.8906\n",
      "Epoch 320/2000\n",
      "8/8 [==============================] - 2s 171ms/step - loss: 0.4626 - accuracy: 0.8392 - val_loss: 0.2648 - val_accuracy: 0.9219\n",
      "Epoch 321/2000\n",
      "8/8 [==============================] - 2s 160ms/step - loss: 0.4648 - accuracy: 0.8196 - val_loss: 0.6691 - val_accuracy: 0.7344\n",
      "Epoch 322/2000\n",
      "8/8 [==============================] - 2s 170ms/step - loss: 0.3960 - accuracy: 0.8431 - val_loss: 0.2129 - val_accuracy: 0.9375\n",
      "Epoch 323/2000\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 0.4153 - accuracy: 0.8588 - val_loss: 0.2971 - val_accuracy: 0.9375\n",
      "Epoch 324/2000\n",
      "8/8 [==============================] - 2s 199ms/step - loss: 0.3402 - accuracy: 0.8941 - val_loss: 0.2731 - val_accuracy: 0.9531\n",
      "Epoch 325/2000\n",
      "8/8 [==============================] - 2s 198ms/step - loss: 0.3912 - accuracy: 0.8549 - val_loss: 0.4801 - val_accuracy: 0.7969\n",
      "Epoch 326/2000\n",
      "8/8 [==============================] - 2s 214ms/step - loss: 0.4905 - accuracy: 0.8235 - val_loss: 0.4048 - val_accuracy: 0.8281\n",
      "Epoch 327/2000\n",
      "8/8 [==============================] - 2s 215ms/step - loss: 0.4010 - accuracy: 0.8627 - val_loss: 0.3322 - val_accuracy: 0.8750\n",
      "Epoch 328/2000\n",
      "8/8 [==============================] - 2s 186ms/step - loss: 0.2997 - accuracy: 0.9098 - val_loss: 0.3210 - val_accuracy: 0.8906\n",
      "Epoch 329/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 0.3435 - accuracy: 0.8941 - val_loss: 0.3634 - val_accuracy: 0.8906\n",
      "Epoch 330/2000\n",
      "8/8 [==============================] - 2s 168ms/step - loss: 0.3219 - accuracy: 0.8824 - val_loss: 0.2918 - val_accuracy: 0.8906\n",
      "Epoch 331/2000\n",
      "8/8 [==============================] - 2s 168ms/step - loss: 0.3023 - accuracy: 0.9059 - val_loss: 0.2527 - val_accuracy: 0.9531\n",
      "Epoch 332/2000\n",
      "8/8 [==============================] - 2s 221ms/step - loss: 0.2835 - accuracy: 0.8980 - val_loss: 0.1871 - val_accuracy: 0.9531\n",
      "Epoch 333/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.3198 - accuracy: 0.8902 - val_loss: 0.2028 - val_accuracy: 0.9531\n",
      "Epoch 334/2000\n",
      "8/8 [==============================] - 2s 171ms/step - loss: 0.3133 - accuracy: 0.9137 - val_loss: 0.2863 - val_accuracy: 0.9062\n",
      "Epoch 335/2000\n",
      "8/8 [==============================] - 2s 160ms/step - loss: 0.3039 - accuracy: 0.9216 - val_loss: 0.2492 - val_accuracy: 0.9375\n",
      "Epoch 336/2000\n",
      "8/8 [==============================] - 2s 159ms/step - loss: 0.2653 - accuracy: 0.9176 - val_loss: 0.1870 - val_accuracy: 0.9531\n",
      "Epoch 337/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.2502 - accuracy: 0.9412 - val_loss: 0.1838 - val_accuracy: 0.9375\n",
      "Epoch 338/2000\n",
      "8/8 [==============================] - 2s 173ms/step - loss: 0.2272 - accuracy: 0.9412 - val_loss: 0.1728 - val_accuracy: 0.9531\n",
      "Epoch 339/2000\n",
      "8/8 [==============================] - 2s 166ms/step - loss: 0.2852 - accuracy: 0.9137 - val_loss: 0.4019 - val_accuracy: 0.8438\n",
      "Epoch 340/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 0.2885 - accuracy: 0.9176 - val_loss: 0.2287 - val_accuracy: 0.9375\n",
      "Epoch 341/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 0.2275 - accuracy: 0.9294 - val_loss: 0.1908 - val_accuracy: 0.9531\n",
      "Epoch 342/2000\n",
      "8/8 [==============================] - 2s 176ms/step - loss: 0.2429 - accuracy: 0.9373 - val_loss: 0.1811 - val_accuracy: 0.9531\n",
      "Epoch 343/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 0.3096 - accuracy: 0.9098 - val_loss: 0.2661 - val_accuracy: 0.8906\n",
      "Epoch 344/2000\n",
      "8/8 [==============================] - 2s 174ms/step - loss: 0.4039 - accuracy: 0.8745 - val_loss: 0.2064 - val_accuracy: 0.9531\n",
      "Epoch 345/2000\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 0.3312 - accuracy: 0.8941 - val_loss: 0.3158 - val_accuracy: 0.8906\n",
      "Epoch 346/2000\n",
      "8/8 [==============================] - 2s 168ms/step - loss: 0.2928 - accuracy: 0.9059 - val_loss: 0.3507 - val_accuracy: 0.9062\n",
      "Epoch 347/2000\n",
      "8/8 [==============================] - 2s 156ms/step - loss: 0.2707 - accuracy: 0.9451 - val_loss: 0.1890 - val_accuracy: 0.9531\n",
      "Epoch 348/2000\n",
      "8/8 [==============================] - 2s 186ms/step - loss: 0.2324 - accuracy: 0.9451 - val_loss: 0.2393 - val_accuracy: 0.9375\n",
      "Epoch 349/2000\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.2773 - accuracy: 0.9137 - val_loss: 0.2769 - val_accuracy: 0.9062\n",
      "Epoch 350/2000\n",
      "8/8 [==============================] - 2s 175ms/step - loss: 0.2827 - accuracy: 0.9216 - val_loss: 0.2703 - val_accuracy: 0.9219\n",
      "Epoch 351/2000\n",
      "8/8 [==============================] - 2s 191ms/step - loss: 0.3370 - accuracy: 0.8863 - val_loss: 0.3054 - val_accuracy: 0.9219\n",
      "Epoch 352/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 0.2674 - accuracy: 0.9137 - val_loss: 0.2117 - val_accuracy: 0.9844\n",
      "Epoch 353/2000\n",
      "8/8 [==============================] - 2s 191ms/step - loss: 0.3387 - accuracy: 0.9098 - val_loss: 0.5322 - val_accuracy: 0.8438\n",
      "Epoch 354/2000\n",
      "8/8 [==============================] - 2s 186ms/step - loss: 0.4022 - accuracy: 0.8706 - val_loss: 0.3559 - val_accuracy: 0.8906\n",
      "Epoch 355/2000\n",
      "8/8 [==============================] - 2s 199ms/step - loss: 0.3506 - accuracy: 0.8941 - val_loss: 0.2862 - val_accuracy: 0.9219\n",
      "Epoch 356/2000\n",
      "8/8 [==============================] - 2s 236ms/step - loss: 0.3298 - accuracy: 0.9020 - val_loss: 0.2204 - val_accuracy: 0.9219\n",
      "Epoch 357/2000\n",
      "8/8 [==============================] - 2s 213ms/step - loss: 0.2727 - accuracy: 0.9294 - val_loss: 0.2044 - val_accuracy: 0.9531\n",
      "Epoch 358/2000\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.2993 - accuracy: 0.9098 - val_loss: 0.3362 - val_accuracy: 0.8750\n",
      "Epoch 359/2000\n",
      "8/8 [==============================] - 2s 236ms/step - loss: 0.3109 - accuracy: 0.8902 - val_loss: 0.3586 - val_accuracy: 0.8750\n",
      "Epoch 360/2000\n",
      "8/8 [==============================] - 2s 217ms/step - loss: 0.2487 - accuracy: 0.9373 - val_loss: 0.2696 - val_accuracy: 0.9219\n",
      "Epoch 361/2000\n",
      "8/8 [==============================] - 2s 177ms/step - loss: 0.2306 - accuracy: 0.9373 - val_loss: 0.2335 - val_accuracy: 0.9375\n",
      "Epoch 362/2000\n",
      "8/8 [==============================] - 2s 192ms/step - loss: 0.2220 - accuracy: 0.9373 - val_loss: 0.2544 - val_accuracy: 0.9375\n",
      "Epoch 363/2000\n",
      "8/8 [==============================] - 2s 213ms/step - loss: 0.2226 - accuracy: 0.9294 - val_loss: 0.3743 - val_accuracy: 0.8438\n",
      "Epoch 364/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 0.2519 - accuracy: 0.9216 - val_loss: 0.1313 - val_accuracy: 1.0000\n",
      "Epoch 365/2000\n",
      "8/8 [==============================] - 2s 186ms/step - loss: 0.2069 - accuracy: 0.9373 - val_loss: 0.1446 - val_accuracy: 0.9688\n",
      "Epoch 366/2000\n",
      "8/8 [==============================] - 2s 180ms/step - loss: 0.2388 - accuracy: 0.9137 - val_loss: 0.2403 - val_accuracy: 0.9219\n",
      "Epoch 367/2000\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 0.2187 - accuracy: 0.9412 - val_loss: 0.2691 - val_accuracy: 0.9219\n",
      "Epoch 368/2000\n",
      "8/8 [==============================] - 2s 193ms/step - loss: 0.2540 - accuracy: 0.9216 - val_loss: 0.2513 - val_accuracy: 0.9375\n",
      "Epoch 369/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.2441 - accuracy: 0.9216 - val_loss: 0.1820 - val_accuracy: 0.9688\n",
      "Epoch 370/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.2664 - accuracy: 0.9020 - val_loss: 0.2496 - val_accuracy: 0.9375\n",
      "Epoch 371/2000\n",
      "8/8 [==============================] - 2s 191ms/step - loss: 0.2359 - accuracy: 0.9333 - val_loss: 0.2929 - val_accuracy: 0.9219\n",
      "Epoch 372/2000\n",
      "8/8 [==============================] - 2s 175ms/step - loss: 0.2735 - accuracy: 0.9137 - val_loss: 0.2889 - val_accuracy: 0.9219\n",
      "Epoch 373/2000\n",
      "8/8 [==============================] - 2s 181ms/step - loss: 0.2267 - accuracy: 0.9412 - val_loss: 0.1533 - val_accuracy: 0.9531\n",
      "Epoch 374/2000\n",
      "8/8 [==============================] - 2s 171ms/step - loss: 0.3737 - accuracy: 0.8902 - val_loss: 0.5922 - val_accuracy: 0.7812\n",
      "Epoch 375/2000\n",
      "8/8 [==============================] - 2s 191ms/step - loss: 0.7074 - accuracy: 0.7647 - val_loss: 0.5418 - val_accuracy: 0.7969\n",
      "Epoch 376/2000\n",
      "8/8 [==============================] - 2s 194ms/step - loss: 0.5254 - accuracy: 0.7922 - val_loss: 0.5421 - val_accuracy: 0.8125\n",
      "Epoch 377/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.7674 - accuracy: 0.7373 - val_loss: 0.5734 - val_accuracy: 0.7500\n",
      "Epoch 378/2000\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.5775 - accuracy: 0.7922 - val_loss: 0.6834 - val_accuracy: 0.7188\n",
      "Epoch 379/2000\n",
      "8/8 [==============================] - 2s 186ms/step - loss: 0.6694 - accuracy: 0.7647 - val_loss: 0.6417 - val_accuracy: 0.7969\n",
      "Epoch 380/2000\n",
      "8/8 [==============================] - 2s 209ms/step - loss: 0.6366 - accuracy: 0.7647 - val_loss: 0.6797 - val_accuracy: 0.7500\n",
      "Epoch 381/2000\n",
      "8/8 [==============================] - 2s 197ms/step - loss: 0.6222 - accuracy: 0.7725 - val_loss: 0.4371 - val_accuracy: 0.8594\n",
      "Epoch 382/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.4662 - accuracy: 0.8353 - val_loss: 0.2746 - val_accuracy: 0.9375\n",
      "Epoch 383/2000\n",
      "8/8 [==============================] - 2s 187ms/step - loss: 0.4398 - accuracy: 0.8392 - val_loss: 0.4259 - val_accuracy: 0.8750\n",
      "Epoch 384/2000\n",
      "8/8 [==============================] - 2s 203ms/step - loss: 0.3204 - accuracy: 0.8745 - val_loss: 0.1869 - val_accuracy: 0.9375\n",
      "Epoch 385/2000\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 0.2211 - accuracy: 0.9294 - val_loss: 0.2418 - val_accuracy: 0.9219\n",
      "Epoch 386/2000\n",
      "8/8 [==============================] - 2s 182ms/step - loss: 0.2297 - accuracy: 0.9451 - val_loss: 0.2716 - val_accuracy: 0.9531\n",
      "Epoch 387/2000\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 0.1887 - accuracy: 0.9451 - val_loss: 0.1936 - val_accuracy: 0.9688\n",
      "Epoch 388/2000\n",
      "8/8 [==============================] - 2s 184ms/step - loss: 0.2018 - accuracy: 0.9569 - val_loss: 0.1677 - val_accuracy: 0.9531\n",
      "Epoch 389/2000\n",
      "8/8 [==============================] - 2s 235ms/step - loss: 0.1929 - accuracy: 0.9373 - val_loss: 0.1451 - val_accuracy: 0.9844\n",
      "Epoch 390/2000\n",
      "8/8 [==============================] - 2s 209ms/step - loss: 0.1838 - accuracy: 0.9373 - val_loss: 0.1167 - val_accuracy: 0.9844\n",
      "Epoch 391/2000\n",
      "8/8 [==============================] - 2s 220ms/step - loss: 0.1430 - accuracy: 0.9765 - val_loss: 0.1389 - val_accuracy: 0.9688\n",
      "Epoch 392/2000\n",
      "8/8 [==============================] - 2s 191ms/step - loss: 0.1471 - accuracy: 0.9804 - val_loss: 0.1264 - val_accuracy: 0.9688\n",
      "Epoch 393/2000\n",
      "8/8 [==============================] - 2s 202ms/step - loss: 0.1451 - accuracy: 0.9725 - val_loss: 0.1454 - val_accuracy: 0.9688\n",
      "Epoch 394/2000\n",
      "8/8 [==============================] - 2s 196ms/step - loss: 0.1502 - accuracy: 0.9647 - val_loss: 0.0885 - val_accuracy: 1.0000\n",
      "Epoch 395/2000\n",
      "8/8 [==============================] - 2s 195ms/step - loss: 0.1849 - accuracy: 0.9412 - val_loss: 0.1048 - val_accuracy: 0.9844\n",
      "Epoch 396/2000\n",
      "8/8 [==============================] - 3s 261ms/step - loss: 0.1469 - accuracy: 0.9765 - val_loss: 0.1016 - val_accuracy: 1.0000\n",
      "Epoch 397/2000\n",
      "8/8 [==============================] - 2s 174ms/step - loss: 0.1732 - accuracy: 0.9569 - val_loss: 0.0993 - val_accuracy: 0.9688\n",
      "Epoch 398/2000\n",
      "8/8 [==============================] - 2s 239ms/step - loss: 0.1614 - accuracy: 0.9725 - val_loss: 0.1042 - val_accuracy: 0.9688\n",
      "Epoch 399/2000\n",
      "8/8 [==============================] - 2s 185ms/step - loss: 0.1456 - accuracy: 0.9647 - val_loss: 0.1531 - val_accuracy: 0.9688\n",
      "Epoch 400/2000\n",
      "8/8 [==============================] - 2s 222ms/step - loss: 0.2111 - accuracy: 0.9373 - val_loss: 0.1407 - val_accuracy: 0.9844\n",
      "Epoch 401/2000\n",
      "8/8 [==============================] - 2s 194ms/step - loss: 0.1480 - accuracy: 0.9725 - val_loss: 0.1255 - val_accuracy: 0.9688\n",
      "Epoch 402/2000\n",
      "8/8 [==============================] - 2s 206ms/step - loss: 0.1111 - accuracy: 0.9882 - val_loss: 0.1220 - val_accuracy: 0.9688\n",
      "Epoch 403/2000\n",
      "8/8 [==============================] - 2s 205ms/step - loss: 0.1475 - accuracy: 0.9647 - val_loss: 0.0980 - val_accuracy: 0.9844\n",
      "Epoch 404/2000\n",
      "8/8 [==============================] - 2s 196ms/step - loss: 0.1188 - accuracy: 0.9725 - val_loss: 0.0987 - val_accuracy: 0.9844\n",
      "Epoch 405/2000\n",
      "8/8 [==============================] - 2s 207ms/step - loss: 0.1572 - accuracy: 0.9608 - val_loss: 0.1976 - val_accuracy: 0.9219\n",
      "Epoch 406/2000\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.1359 - accuracy: 0.9765 - val_loss: 0.0781 - val_accuracy: 1.0000\n",
      "Epoch 407/2000\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.1515 - accuracy: 0.9647 - val_loss: 0.1682 - val_accuracy: 0.9531\n",
      "Epoch 408/2000\n",
      "8/8 [==============================] - 2s 186ms/step - loss: 0.1129 - accuracy: 0.9882 - val_loss: 0.1199 - val_accuracy: 0.9844\n",
      "Epoch 409/2000\n",
      "8/8 [==============================] - 2s 194ms/step - loss: 0.1312 - accuracy: 0.9804 - val_loss: 0.1054 - val_accuracy: 0.9844\n",
      "Epoch 410/2000\n",
      "8/8 [==============================] - 2s 192ms/step - loss: 0.1406 - accuracy: 0.9686 - val_loss: 0.1713 - val_accuracy: 0.9688\n",
      "Epoch 411/2000\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.1621 - accuracy: 0.9598"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6336/3052477205.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtb_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\FYP\\sign-recognition\\ActionDetectionforSignLanguage-main\\ActionDetectionforSignLanguage-main\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP\\sign-recognition\\ActionDetectionforSignLanguage-main\\ActionDetectionforSignLanguage-main\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP\\sign-recognition\\ActionDetectionforSignLanguage-main\\ActionDetectionforSignLanguage-main\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP\\sign-recognition\\ActionDetectionforSignLanguage-main\\ActionDetectionforSignLanguage-main\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP\\sign-recognition\\ActionDetectionforSignLanguage-main\\ActionDetectionforSignLanguage-main\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP\\sign-recognition\\ActionDetectionforSignLanguage-main\\ActionDetectionforSignLanguage-main\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP\\sign-recognition\\ActionDetectionforSignLanguage-main\\ActionDetectionforSignLanguage-main\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32md:\\FYP\\sign-recognition\\ActionDetectionforSignLanguage-main\\ActionDetectionforSignLanguage-main\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\FYP\\sign-recognition\\ActionDetectionforSignLanguage-main\\ActionDetectionforSignLanguage-main\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('steven{}.h5'.format(TRIAL+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52ee2977380704a66854748a73250e0671a9318bd5b3fd45a3df9f851ae61629"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
